# -*- coding: utf-8 -*-
"""LastProjectGa

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14mBo1GdWTQbRDu8PxPXnfkCiPDhEx21C
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile file.cpp
# #include "ns3/core-module.h"
# #include "ns3/network-module.h"
# #include "ns3/internet-module.h"
# #include "ns3/point-to-point-module.h"
# #include "ns3/applications-module.h"
# #include "ns3/ipv4-global-routing-helper.h"
# 
# using namespace ns3;
# 
# int main() {
#   // Create nodes
#   NodeContainer clients;
#   clients.Create(4);
# 
#   NodeContainer servers;
#   servers.Create(2);
# 
#   NodeContainer hub;
#   hub.Create(1);  // Central router
# 
#   //Topology
#   //:Clients (0–3) connect to the hub (node 4) via fast links (10Mbps, 2ms delay).
#   //Servers (5–6) connect to the hub via slower links (2Mbps, 10ms delay).
# 
#   // Create channels
#   PointToPointHelper p2p;
#   p2p.SetDeviceAttribute("DataRate", StringValue("10Mbps"));
#   p2p.SetChannelAttribute("Delay", StringValue("2ms"));
# 
#   PointToPointHelper slowP2P;
#   slowP2P.SetDeviceAttribute("DataRate", StringValue("2Mbps"));
#   slowP2P.SetChannelAttribute("Delay", StringValue("10ms"));
# 
#   NetDeviceContainer clientDevices[4];
#   NetDeviceContainer serverDevices[2];
#   NetDeviceContainer hubDevices[6];
# 
# //Hub as Router : The hub node acts as a router with 6 interfaces (4 clients + 2 servers).
# 
#   // Connect clients to hub
#   for (int i = 0; i < 4; ++i) {
#     NodeContainer pair = NodeContainer(clients.Get(i), hub.Get(0));
#     NetDeviceContainer devices = p2p.Install(pair);
# 
#     clientDevices[i] = devices.Get(0);
#     hubDevices[i] = devices.Get(1);
#   }
# 
#   // Connect servers to hub
#   for (int i = 0; i < 2; ++i) {
#     NodeContainer pair = NodeContainer(servers.Get(i), hub.Get(0));
#     NetDeviceContainer devices = slowP2P.Install(pair);
# 
#     serverDevices[i] = devices.Get(0);
#     hubDevices[4+i] = devices.Get(1);
#   }
# 
#   // Install internet stack
#   InternetStackHelper stack;
#   stack.InstallAll();
# 
#   // Assign IP addresses
#   Ipv4AddressHelper address;
#   Ipv4InterfaceContainer interfaces[6];
# 
#   for (int i = 0; i < 6; ++i) {
#     std::ostringstream subnet;
#     subnet << "10.1." << i+1 << ".0";
#     address.SetBase(subnet.str().c_str(), "255.255.255.0");
#     interfaces[i] = address.Assign(hubDevices[i]);
#   }
# 
#   // Set up global routing
#   Ipv4GlobalRoutingHelper::PopulateRoutingTables();
# 
#   // Install applications
#   // UDP Echo Server on server 0
#   UdpEchoServerHelper echoServer(9);
#   ApplicationContainer serverApp = echoServer.Install(servers.Get(0));
#   serverApp.Start(Seconds(1.0));
#   serverApp.Stop(Seconds(10.0));
# 
#   // BulkSendApplication on server 1
#   BulkSendHelper bulkSender("ns3::TcpSocketFactory",
#                             InetSocketAddress(interfaces[5].GetAddress(0), 80));
#   bulkSender.SetAttribute("MaxBytes", UintegerValue(10485760)); // 10MB
#   ApplicationContainer bulkApp = bulkSender.Install(servers.Get(1));
#   bulkApp.Start(Seconds(2.0));
#   bulkApp.Stop(Seconds(10.0));
# 
#   // UDP Clients
#   for (int i = 0; i < 4; ++i) {
#     UdpEchoClientHelper echoClient(interfaces[4].GetAddress(0), 9);
#     echoClient.SetAttribute("MaxPackets", UintegerValue(5));
#     echoClient.SetAttribute("Interval", TimeValue(Seconds(1.0)));
#     echoClient.SetAttribute("PacketSize", UintegerValue(1024));
# 
#     ApplicationContainer clientApp = echoClient.Install(clients.Get(i));
#     clientApp.Start(Seconds(2.0 + i*0.5));
#     clientApp.Stop(Seconds(10.0));
#   }
# 
#   Simulator::Run();
#   Simulator::Destroy();
#   return 0;
# }
# //Purpose : Imports ns-3 modules for simulation core, networking, IP protocols, point-to-point links, applications (like UDP/TCP), and global routing.

# Re-clone the repository
#!git clone https://gitlab.com/nsnam/ns-3-dev.git  ns-3

!sudo apt-get update
!sudo apt-get install -y build-essential python3-pip git

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def simulate_network_metrics(num_samples=1000, random_seed=42):
    np.random.seed(random_seed)
    time_index = pd.date_range(start='2024-01-01', periods=num_samples, freq='1min')

    # Base metrics
    throughput = np.random.normal(100, 15, num_samples)
    latency = np.random.normal(20, 5, num_samples)
    packet_loss = np.random.uniform(0, 0.05, num_samples)
    active_users = np.random.randint(50, 200, num_samples)
    bandwidth_util = np.random.uniform(0.4, 0.9, num_samples)

    # Time-based patterns
    business_hours = (time_index.hour >= 9) & (time_index.hour <= 17)
    throughput[business_hours] *= 1.3
    active_users = active_users.astype(float)
    active_users[business_hours] *= 1.5
    active_users = active_users.astype(int)

    return pd.DataFrame({
        'timestamp': time_index,
        'throughput': throughput,
        'latency': latency,
        'packet_loss': packet_loss,
        'active_users': active_users,
        'bandwidth_utilization': bandwidth_util
    })

def add_derived_metrics(df):
    df['throughput_ma'] = df['throughput'].rolling(window=10).mean()
    df['latency_ma'] = df['latency'].rolling(window=10).mean()
    throughput_threshold = 70.0
    df['congestion'] = (df['throughput'] < throughput_threshold).astype(int)
    return df.dropna()

# Generate and save dataset
if __name__ == "__main__":
    data = add_derived_metrics(simulate_network_metrics(10000))
    data.to_csv('network_metrics.csv', index=False)

# --- Visualization 1: Time Series Plots ---
data = pd.read_csv('network_metrics.csv')
metrics = ['throughput', 'latency', 'packet_loss', 'active_users', 'bandwidth_utilization']

# --- Visualization 2: Correlation Heatmap ---
corr = data[metrics].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title('Correlation Between Network Metrics')
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Example: Simulated UDP RTT data
rtt_data = {
    "Client": ["Client 0", "Client 1", "Client 2", "Client 3"],
    "RTT (ms)": [2.5, 3.0, 2.8, 3.2]  # Replace with actual ns-3 logs
}

plt.figure(figsize=(8, 4))
sns.barplot(x="Client", y="RTT (ms)", data=rtt_data)
plt.title("UDP Echo Round-Trip Time (RTT)")
plt.ylabel("RTT (ms)")
plt.xlabel("Client")
plt.grid(True)
plt.show()

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay
import pickle  # <-- Added missing import

class NetworkCongestionPredictor:
    def __init__(self, lookback_window=60):
        self.lookback_window = lookback_window
        self.scaler = StandardScaler()
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential([
            LSTM(32, input_shape=(self.lookback_window, 5)),
            Dropout(0.2),
            Dense(8, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def prepare_data(self, data):
        features = ['throughput', 'latency', 'packet_loss', 'active_users', 'bandwidth_utilization']
        X = data[features].values
        X_scaled = self.scaler.fit_transform(X)

        X_sequences, y = [], []
        for i in range(len(X_scaled) - self.lookback_window):
            X_sequences.append(X_scaled[i:i+self.lookback_window])
            y.append(data.iloc[i+self.lookback_window]['congestion'])
        return np.array(X_sequences), np.array(y)

    def train(self, X_train, y_train):
        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
        return self.model.fit(X_train, y_train,
                             epochs=50,
                             batch_size=32,
                             validation_split=0.2,
                             callbacks=[early_stop],
                             verbose=1)

# Training pipeline
if __name__ == "__main__":
    data = pd.read_csv('network_metrics.csv')
    predictor = NetworkCongestionPredictor()
    X, y = predictor.prepare_data(data)

    split_idx = int(0.8 * len(X))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    history = predictor.train(X_train, y_train)
    predictor.model.save('network_congestion_model.keras')
    with open('scaler.pkl', 'wb') as f:
        pickle.dump(predictor.scaler, f)  # Now works correctly

# --- Visualization 3: Model Training Curves ---
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.suptitle('Model Training Performance')
plt.show()

# --- Visualization 4: ROC Curve ---
y_pred = predictor.model.predict(X_test).flatten()
fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend()
plt.show()

data.head(5)

import numpy as np
import tensorflow as tf
import pickle
import matplotlib.pyplot as plt

def load_prediction_components():
    model = tf.keras.models.load_model('network_congestion_model.keras')
    with open('scaler.pkl', 'rb') as f:
        scaler = pickle.load(f)
    return model, scaler

def predict_congestion(model, scaler, metrics, window=60):
    if metrics.shape[0] == 1:
        metrics = np.tile(metrics, (window, 1))
    scaled = scaler.transform(metrics)
    return model.predict(scaled.reshape(1, window, 5), verbose=0)[0][0]

# --- Visualization 6: Test Case Results ---
if __name__ == "__main__":
    model, scaler = load_prediction_components()

    test_cases = {
        "Normal Traffic": [110, 20, 0.01, 120, 0.6],
        "Congested":      [30, 35, 0.05, 250, 0.95]
    }

    labels = list(test_cases.keys())
    probs = []
    for label, metrics in test_cases.items():
        prob = predict_congestion(model, scaler, np.array([metrics]))
        probs.append(prob)
        print(f"{label} Scenario: {prob:.2%} congestion probability")

    plt.figure(figsize=(6, 4))
    plt.bar(labels, probs, color=['green', 'red'])
    plt.ylabel('Congestion Probability')
    plt.title('Congestion Prediction for Test Scenarios')
    plt.show()

